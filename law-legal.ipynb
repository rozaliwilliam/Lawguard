{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9384625,"sourceType":"datasetVersion","datasetId":5693742},{"sourceId":104433,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68806,"modelId":91102}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\n\nfile_path = \"/kaggle/input/crime-dataset/qa_model_tanzania_crime.json\"\n\n# Load the JSON data\nwith open(file_path, 'r') as f:\n    qa_data = json.load(f)\n\n# Inspect the data\nprint(qa_data[:5])  # This will print the first 5 entries in the dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:41:24.453250Z","iopub.execute_input":"2024-09-13T14:41:24.453504Z","iopub.status.idle":"2024-09-13T14:41:24.472697Z","shell.execute_reply.started":"2024-09-13T14:41:24.453471Z","shell.execute_reply":"2024-09-13T14:41:24.472009Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[{'context': 'Article 13 of the Constitution of Tanzania states that all persons are equal before the law and are entitled, without discrimination, to protection and equality before the law.', 'question': 'What does the Tanzanian Constitution say about equality before the law in criminal cases?', 'answer': 'All persons are equal before the law and are entitled to protection and equality before the law.', 'source': 'Article 13, Constitution of Tanzania'}, {'context': 'Article 30 of the Constitution limits the exercise of individual rights and freedoms if they infringe upon the rights and freedoms of others or affect public safety and order.', 'question': 'Can individual rights be limited in Tanzania if they threaten public safety?', 'answer': 'Yes, individual rights can be limited if they threaten public safety and order.', 'source': 'Article 30, Constitution of Tanzania'}, {'context': 'Article 31 of the Constitution discusses derogation from rights and freedoms in situations of emergency, such as when a person engages in activities that endanger national security.', 'question': 'Can rights and freedoms be derogated in Tanzania during a state of emergency due to national security threats?', 'answer': 'Yes, rights and freedoms can be derogated in situations that threaten national security.', 'source': 'Article 31, Constitution of Tanzania'}, {'context': 'Article 45 of the Constitution gives the President the power to pardon any person convicted by a court of law.', 'question': 'Does the President of Tanzania have the power to pardon convicted criminals?', 'answer': 'Yes, the President has the power to pardon convicted criminals.', 'source': 'Article 45, Constitution of Tanzania'}, {'context': 'Article 28 of the Constitution imposes a duty on citizens to defend and protect the nation from any threats, including crimes against the state.', 'question': 'What duty does the Tanzanian Constitution impose on citizens regarding national defense and protection?', 'answer': 'Citizens have the duty to defend and protect the nation from any threats, including crimes against the state.', 'source': 'Article 28, Constitution of Tanzania'}]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame(qa_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:41:24.473485Z","iopub.execute_input":"2024-09-13T14:41:24.473728Z","iopub.status.idle":"2024-09-13T14:41:26.239696Z","shell.execute_reply.started":"2024-09-13T14:41:24.473702Z","shell.execute_reply":"2024-09-13T14:41:26.238908Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:41:26.240661Z","iopub.execute_input":"2024-09-13T14:41:26.241004Z","iopub.status.idle":"2024-09-13T14:41:26.255293Z","shell.execute_reply.started":"2024-09-13T14:41:26.240975Z","shell.execute_reply":"2024-09-13T14:41:26.254598Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                             context  \\\n0  Article 13 of the Constitution of Tanzania sta...   \n1  Article 30 of the Constitution limits the exer...   \n2  Article 31 of the Constitution discusses derog...   \n3  Article 45 of the Constitution gives the Presi...   \n4  Article 28 of the Constitution imposes a duty ...   \n\n                                            question  \\\n0  What does the Tanzanian Constitution say about...   \n1  Can individual rights be limited in Tanzania i...   \n2  Can rights and freedoms be derogated in Tanzan...   \n3  Does the President of Tanzania have the power ...   \n4  What duty does the Tanzanian Constitution impo...   \n\n                                              answer  \\\n0  All persons are equal before the law and are e...   \n1  Yes, individual rights can be limited if they ...   \n2  Yes, rights and freedoms can be derogated in s...   \n3  Yes, the President has the power to pardon con...   \n4  Citizens have the duty to defend and protect t...   \n\n                                 source  \n0  Article 13, Constitution of Tanzania  \n1  Article 30, Constitution of Tanzania  \n2  Article 31, Constitution of Tanzania  \n3  Article 45, Constitution of Tanzania  \n4  Article 28, Constitution of Tanzania  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Article 13 of the Constitution of Tanzania sta...</td>\n      <td>What does the Tanzanian Constitution say about...</td>\n      <td>All persons are equal before the law and are e...</td>\n      <td>Article 13, Constitution of Tanzania</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Article 30 of the Constitution limits the exer...</td>\n      <td>Can individual rights be limited in Tanzania i...</td>\n      <td>Yes, individual rights can be limited if they ...</td>\n      <td>Article 30, Constitution of Tanzania</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Article 31 of the Constitution discusses derog...</td>\n      <td>Can rights and freedoms be derogated in Tanzan...</td>\n      <td>Yes, rights and freedoms can be derogated in s...</td>\n      <td>Article 31, Constitution of Tanzania</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Article 45 of the Constitution gives the Presi...</td>\n      <td>Does the President of Tanzania have the power ...</td>\n      <td>Yes, the President has the power to pardon con...</td>\n      <td>Article 45, Constitution of Tanzania</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Article 28 of the Constitution imposes a duty ...</td>\n      <td>What duty does the Tanzanian Constitution impo...</td>\n      <td>Citizens have the duty to defend and protect t...</td>\n      <td>Article 28, Constitution of Tanzania</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:41:26.257453Z","iopub.execute_input":"2024-09-13T14:41:26.257748Z","iopub.status.idle":"2024-09-13T14:41:30.463132Z","shell.execute_reply.started":"2024-09-13T14:41:26.257719Z","shell.execute_reply":"2024-09-13T14:41:30.462103Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (1.5.1)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.14.0)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, test_size = 0.1, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:41:30.464880Z","iopub.execute_input":"2024-09-13T14:41:30.465623Z","iopub.status.idle":"2024-09-13T14:41:32.072898Z","shell.execute_reply.started":"2024-09-13T14:41:30.465564Z","shell.execute_reply":"2024-09-13T14:41:32.072024Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Save the train and test sets\ntrain_df.to_json('train_data.json', orient='records', lines=True)\ntest_df.to_json('test_data.json', orient='records', lines=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:41:32.073992Z","iopub.execute_input":"2024-09-13T14:41:32.074353Z","iopub.status.idle":"2024-09-13T14:41:32.079753Z","shell.execute_reply.started":"2024-09-13T14:41:32.074324Z","shell.execute_reply":"2024-09-13T14:41:32.079031Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install transformers datasets accelerate","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:41:32.080708Z","iopub.execute_input":"2024-09-13T14:41:32.080982Z","iopub.status.idle":"2024-09-13T14:41:40.243616Z","shell.execute_reply.started":"2024-09-13T14:41:32.080953Z","shell.execute_reply":"2024-09-13T14:41:40.242623Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.44.0)\nCollecting datasets\n  Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/site-packages (0.33.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2024.7.24)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.15.4)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.66.5)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.2.2)\nCollecting multiprocess\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xxhash\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.3.8)\nCollecting aiohttp\n  Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from datasets) (2024.6.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate) (6.0.0)\nCollecting aiohappyeyeballs>=2.3.0\n  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-timeout<5.0,>=4.0\n  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting yarl<2.0,>=1.0\n  Downloading yarl-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.8/446.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\nRequirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.20)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nInstalling collected packages: xxhash, multiprocess, multidict, frozenlist, async-timeout, aiohappyeyeballs, yarl, aiosignal, aiohttp, datasets\nSuccessfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.0 frozenlist-1.4.1 multidict-6.1.0 multiprocess-0.70.16 xxhash-3.5.0 yarl-1.11.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nfrom datasets import load_dataset\n\n# Load the dataset from JSON files\ndata_files = {\n    \"train\": \"train_data.json\",\n    \"test\": \"test_data.json\"\n}\n\ndataset = load_dataset(\"json\", data_files=data_files)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:41:40.245065Z","iopub.execute_input":"2024-09-13T14:41:40.245355Z","iopub.status.idle":"2024-09-13T14:41:41.029677Z","shell.execute_reply.started":"2024-09-13T14:41:40.245326Z","shell.execute_reply":"2024-09-13T14:41:41.028992Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nGenerating train split: 17 examples [00:00, 4262.76 examples/s]\nGenerating test split: 2 examples [00:00, 1101.01 examples/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:41:41.030498Z","iopub.execute_input":"2024-09-13T14:41:41.030846Z","iopub.status.idle":"2024-09-13T14:41:41.034744Z","shell.execute_reply.started":"2024-09-13T14:41:41.030818Z","shell.execute_reply":"2024-09-13T14:41:41.034078Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['context', 'question', 'answer', 'source'],\n        num_rows: 17\n    })\n    test: Dataset({\n        features: ['context', 'question', 'answer', 'source'],\n        num_rows: 2\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import LlamaTokenizerFast\n\n# Load the LLaMA tokenizer\ntokenizer = LlamaTokenizerFast.from_pretrained('/kaggle/input/llama-3.1/transformers/8b/2')\n\n# Set the pad_token to eos_token\ntokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"context\"], padding=\"max_length\", truncation=True, max_length=512)\n\n# Apply the tokenizer to the dataset\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:43:37.553630Z","iopub.execute_input":"2024-09-13T14:43:37.554065Z","iopub.status.idle":"2024-09-13T14:43:38.395395Z","shell.execute_reply.started":"2024-09-13T14:43:37.554029Z","shell.execute_reply":"2024-09-13T14:43:38.394600Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \nThe class this function is called from is 'LlamaTokenizerFast'.\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\nMap: 100%|██████████| 17/17 [00:00<00:00, 474.00 examples/s]\nMap: 100%|██████████| 2/2 [00:00<00:00, 309.12 examples/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import LlamaForCausalLM\n\n# Load the LLaMA 3.1 8B model\nmodel = LlamaForCausalLM.from_pretrained('/kaggle/input/llama-3.1/transformers/8b/2') ","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:43:59.130451Z","iopub.execute_input":"2024-09-13T14:43:59.131288Z","iopub.status.idle":"2024-09-13T14:44:30.260185Z","shell.execute_reply.started":"2024-09-13T14:43:59.131251Z","shell.execute_reply":"2024-09-13T14:44:30.259144Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.72s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch-xla","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:58:03.098721Z","iopub.execute_input":"2024-09-13T14:58:03.099041Z","iopub.status.idle":"2024-09-13T14:58:07.675847Z","shell.execute_reply.started":"2024-09-13T14:58:03.099013Z","shell.execute_reply":"2024-09-13T14:58:07.674651Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch-xla in /usr/local/lib/python3.10/site-packages (2.4.0+libtpu)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from torch-xla) (2.1.0)\nRequirement already satisfied: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.10/site-packages (from torch-xla) (0.10)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from torch-xla) (6.0.2)\nRequirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla) (1.8.0)\nRequirement already satisfied: oauth2client in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla) (4.1.3)\nRequirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.34.1)\nRequirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.34.0)\nRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.16.0)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.2.0)\nRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.22.0)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.0.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.4.0)\nRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (4.9)\nRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.6.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.20.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.32.3)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.63.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (5.5.0)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.1.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2024.7.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.2.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.3.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nfrom transformers import TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2024-09-13T15:06:14.352081Z","iopub.execute_input":"2024-09-13T15:06:14.352407Z","iopub.status.idle":"2024-09-13T15:06:14.357108Z","shell.execute_reply.started":"2024-09-13T15:06:14.352379Z","shell.execute_reply":"2024-09-13T15:06:14.356182Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    use_ipex=False,  # Disable Intel Extension for PyTorch\n    fp16=False,  # Disable fp16 as it's not supported on TPU\n    bf16=True,  # Use bfloat16 instead, which is supported on TPU\n    tpu_num_cores=8,  # Number of TPU cores\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T15:07:15.869872Z","iopub.execute_input":"2024-09-13T15:07:15.870620Z","iopub.status.idle":"2024-09-13T15:07:15.876860Z","shell.execute_reply.started":"2024-09-13T15:07:15.870581Z","shell.execute_reply":"2024-09-13T15:07:15.875946Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def train_func(index):\n    # Set the device\n    device = xm.xla_device()\n    \n    # Initialize your model (replace with your actual model initialization)\n    model = YourModelClass().to(device)\n    \n    # Create the Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets['train'],\n        eval_dataset=tokenized_datasets['test']\n    )\n    \n    # Train the model\n    trainer.train()\n    \n    # Save the model\n    xm.save(trainer.model.state_dict(), 'path_to_save_model.bin')","metadata":{"execution":{"iopub.status.busy":"2024-09-13T15:07:43.482790Z","iopub.execute_input":"2024-09-13T15:07:43.483132Z","iopub.status.idle":"2024-09-13T15:07:43.487885Z","shell.execute_reply.started":"2024-09-13T15:07:43.483104Z","shell.execute_reply":"2024-09-13T15:07:43.487026Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    xmp.spawn(train_func, nprocs=8)  # Start 8 processes, one for each TPU core","metadata":{"execution":{"iopub.status.busy":"2024-09-13T15:08:19.011412Z","iopub.execute_input":"2024-09-13T15:08:19.011774Z","iopub.status.idle":"2024-09-13T15:08:19.111611Z","shell.execute_reply.started":"2024-09-13T15:08:19.011743Z","shell.execute_reply":"2024-09-13T15:08:19.110616Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"WARNING:root:Unsupported nprocs (8), ignoring...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mxmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Start 8 processes, one for each TPU core\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/distributed/xla_multiprocessing.py:38\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@xr\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_pjrt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawn\u001b[39m(fn,\n\u001b[1;32m      8\u001b[0m           args\u001b[38;5;241m=\u001b[39m(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m           daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m           start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Enables multi processing based replication.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    return None.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpjrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:214\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nprocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported nprocs (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), ignoring...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m nprocs)\n\u001b[0;32m--> 214\u001b[0m \u001b[43mrun_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:150\u001b[0m, in \u001b[0;36mrun_multiprocess\u001b[0;34m(fn, start_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs `fn` on all devices available to PjRt.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03mSpawns one process per physical device (e.g. TPU chip).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m  return_value is the result of calling `fn`.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_xla\u001b[38;5;241m.\u001b[39m_XLAC\u001b[38;5;241m.\u001b[39m_xla_runtime_is_initialized():\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRuntime is already initialized. Do not use the XLA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    151\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice before calling xmp.spawn.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plugins\u001b[38;5;241m.\u001b[39musing_dynamic_plugins():\n\u001b[1;32m    154\u001b[0m   num_processes \u001b[38;5;241m=\u001b[39m plugins\u001b[38;5;241m.\u001b[39mdefault()\u001b[38;5;241m.\u001b[39mphysical_chip_count()\n","\u001b[0;31mRuntimeError\u001b[0m: Runtime is already initialized. Do not use the XLA device before calling xmp.spawn."],"ename":"RuntimeError","evalue":"Runtime is already initialized. Do not use the XLA device before calling xmp.spawn.","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nfrom transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n\ndef train_func(index):\n    # Set the device\n    #device = xm.xla_device()\n    \n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/llama-3.1/transformers/8b/2')\n    model = AutoModelForSequenceClassification.from_pretrained('/kaggle/input/llama-3.1/transformers/8b/2').to(device)\n    \n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        num_train_epochs=3,\n        use_ipex=False,\n        fp16=False,\n        bf16=True,\n        tpu_num_cores=8,\n    )\n    \n    \n    # Create the Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets['train'],\n        eval_dataset=tokenized_datasets['test'],\n    )\n    \n    # Train the model\n    trainer.train()\n    \n    # Save the model on the main process\n    if xm.is_master_ordinal():\n        trainer.save_model(\"./saved_model\")\n        tokenizer.save_pretrained(\"./saved_model\")\n\ndef main():\n    # Start the multiprocessing\n    xmp.spawn(train_func, nprocs=8)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-09-13T15:18:16.722011Z","iopub.execute_input":"2024-09-13T15:18:16.722346Z","iopub.status.idle":"2024-09-13T15:18:16.816573Z","shell.execute_reply.started":"2024-09-13T15:18:16.722318Z","shell.execute_reply":"2024-09-13T15:18:16.815412Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"WARNING:root:Unsupported nprocs (8), ignoring...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     xmp\u001b[38;5;241m.\u001b[39mspawn(train_func, nprocs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[45], line 46\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Start the multiprocessing\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mxmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/distributed/xla_multiprocessing.py:38\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@xr\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_pjrt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawn\u001b[39m(fn,\n\u001b[1;32m      8\u001b[0m           args\u001b[38;5;241m=\u001b[39m(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m           daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m           start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Enables multi processing based replication.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    return None.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpjrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:214\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nprocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported nprocs (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), ignoring...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m nprocs)\n\u001b[0;32m--> 214\u001b[0m \u001b[43mrun_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:150\u001b[0m, in \u001b[0;36mrun_multiprocess\u001b[0;34m(fn, start_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs `fn` on all devices available to PjRt.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03mSpawns one process per physical device (e.g. TPU chip).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m  return_value is the result of calling `fn`.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_xla\u001b[38;5;241m.\u001b[39m_XLAC\u001b[38;5;241m.\u001b[39m_xla_runtime_is_initialized():\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRuntime is already initialized. Do not use the XLA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    151\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice before calling xmp.spawn.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plugins\u001b[38;5;241m.\u001b[39musing_dynamic_plugins():\n\u001b[1;32m    154\u001b[0m   num_processes \u001b[38;5;241m=\u001b[39m plugins\u001b[38;5;241m.\u001b[39mdefault()\u001b[38;5;241m.\u001b[39mphysical_chip_count()\n","\u001b[0;31mRuntimeError\u001b[0m: Runtime is already initialized. Do not use the XLA device before calling xmp.spawn."],"ename":"RuntimeError","evalue":"Runtime is already initialized. Do not use the XLA device before calling xmp.spawn.","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nfrom transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n\ndef train_func(index):\n    # Set the device\n    device = xm.xla_device()\n    \n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/llama-3.1/transformers/8b/2')\n    model = AutoModelForSequenceClassification.from_pretrained('/kaggle/input/llama-3.1/transformers/8b/2').to(device)\n    \n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        num_train_epochs=3,\n        use_ipex=False,\n        fp16=False,\n        bf16=True,\n        tpu_num_cores=8,\n    )\n    \n    # Load and tokenize your datasets here\n    # This is where you should prepare your tokenized_datasets\n    tokenized_datasets = prepare_datasets(tokenizer)\n    \n    # Create the Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets['train'],\n        eval_dataset=tokenized_datasets['test'],\n    )\n    \n    # Train the model\n    trainer.train()\n    \n    # Save the model on the main process\n    if xm.is_master_ordinal():\n        trainer.save_model(\"./saved_model\")\n        tokenizer.save_pretrained(\"./saved_model\")\n\ndef prepare_datasets(tokenizer):\n    # Implement your dataset preparation logic here\n    # Make sure not to use any XLA operations in this function\n    # Return a dictionary with 'train' and 'test' datasets\n    pass\n\ndef main():\n    # Start the multiprocessing\n    xmp.spawn(train_func, nprocs=8)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-09-13T15:21:32.812648Z","iopub.execute_input":"2024-09-13T15:21:32.813312Z","iopub.status.idle":"2024-09-13T15:21:32.906524Z","shell.execute_reply.started":"2024-09-13T15:21:32.813276Z","shell.execute_reply":"2024-09-13T15:21:32.905314Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"WARNING:root:Unsupported nprocs (8), ignoring...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m     xmp\u001b[38;5;241m.\u001b[39mspawn(train_func, nprocs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[46], line 55\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Start the multiprocessing\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mxmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/distributed/xla_multiprocessing.py:38\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@xr\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_pjrt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawn\u001b[39m(fn,\n\u001b[1;32m      8\u001b[0m           args\u001b[38;5;241m=\u001b[39m(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m           daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m           start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Enables multi processing based replication.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    return None.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpjrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:214\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nprocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported nprocs (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), ignoring...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m nprocs)\n\u001b[0;32m--> 214\u001b[0m \u001b[43mrun_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:150\u001b[0m, in \u001b[0;36mrun_multiprocess\u001b[0;34m(fn, start_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs `fn` on all devices available to PjRt.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03mSpawns one process per physical device (e.g. TPU chip).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m  return_value is the result of calling `fn`.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_xla\u001b[38;5;241m.\u001b[39m_XLAC\u001b[38;5;241m.\u001b[39m_xla_runtime_is_initialized():\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRuntime is already initialized. Do not use the XLA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    151\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice before calling xmp.spawn.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plugins\u001b[38;5;241m.\u001b[39musing_dynamic_plugins():\n\u001b[1;32m    154\u001b[0m   num_processes \u001b[38;5;241m=\u001b[39m plugins\u001b[38;5;241m.\u001b[39mdefault()\u001b[38;5;241m.\u001b[39mphysical_chip_count()\n","\u001b[0;31mRuntimeError\u001b[0m: Runtime is already initialized. Do not use the XLA device before calling xmp.spawn."],"ename":"RuntimeError","evalue":"Runtime is already initialized. Do not use the XLA device before calling xmp.spawn.","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n\ndef train_func(index):\n    # Import XLA modules only inside the function\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n\n    # Set the device\n    device = xm.xla_device()\n    \n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/llama-3.1/transformers/8b/2')\n    model = AutoModelForSequenceClassification.from_pretrained('/kaggle/input/llama-3.1/transformers/8b/2').to(device)\n    \n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        num_train_epochs=3,\n        use_ipex=False,\n        fp16=False,\n        bf16=True,\n        tpu_num_cores=8,\n    )\n    \n    # Load and tokenize your datasets here\n    tokenized_datasets = prepare_datasets(tokenizer)\n    \n    # Create the Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets['train'],\n        eval_dataset=tokenized_datasets['test'],\n    )\n    \n    # Train the model\n    trainer.train()\n    \n    # Save the model on the main process\n    if xm.is_master_ordinal():\n        trainer.save_model(\"./saved_model\")\n        tokenizer.save_pretrained(\"./saved_model\")\n\ndef prepare_datasets(tokenizer):\n    # Implement your dataset preparation logic here\n    # Make sure not to use any XLA operations in this function\n    # Return a dictionary with 'train' and 'test' datasets\n    pass\n\nif __name__ == \"__main__\":\n    # Import XLA multiprocessing module only when needed\n    import torch_xla.distributed.xla_multiprocessing as xmp\n    xmp.spawn(train_func, nprocs=8)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T15:24:46.978619Z","iopub.execute_input":"2024-09-13T15:24:46.978987Z","iopub.status.idle":"2024-09-13T15:24:47.055400Z","shell.execute_reply.started":"2024-09-13T15:24:46.978958Z","shell.execute_reply":"2024-09-13T15:24:47.054358Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"WARNING:root:Unsupported nprocs (8), ignoring...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Import XLA multiprocessing module only when needed\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla_multiprocessing\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxmp\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mxmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/distributed/xla_multiprocessing.py:38\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@xr\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_pjrt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawn\u001b[39m(fn,\n\u001b[1;32m      8\u001b[0m           args\u001b[38;5;241m=\u001b[39m(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m           daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m           start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Enables multi processing based replication.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    return None.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpjrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:214\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nprocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported nprocs (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), ignoring...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m nprocs)\n\u001b[0;32m--> 214\u001b[0m \u001b[43mrun_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:150\u001b[0m, in \u001b[0;36mrun_multiprocess\u001b[0;34m(fn, start_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs `fn` on all devices available to PjRt.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03mSpawns one process per physical device (e.g. TPU chip).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m  return_value is the result of calling `fn`.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_xla\u001b[38;5;241m.\u001b[39m_XLAC\u001b[38;5;241m.\u001b[39m_xla_runtime_is_initialized():\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRuntime is already initialized. Do not use the XLA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    151\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice before calling xmp.spawn.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plugins\u001b[38;5;241m.\u001b[39musing_dynamic_plugins():\n\u001b[1;32m    154\u001b[0m   num_processes \u001b[38;5;241m=\u001b[39m plugins\u001b[38;5;241m.\u001b[39mdefault()\u001b[38;5;241m.\u001b[39mphysical_chip_count()\n","\u001b[0;31mRuntimeError\u001b[0m: Runtime is already initialized. Do not use the XLA device before calling xmp.spawn."],"ename":"RuntimeError","evalue":"Runtime is already initialized. Do not use the XLA device before calling xmp.spawn.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}